# docker-compose.yml
# Compose file to bring up Zookeeper, Kafka, Hadoop (namenode/datanode), Spark master/worker,
# MongoDB, backend and frontend for the crypto-bigdata-pipeline demo.
services:
  zookeeper:
    # Use Confluent Zookeeper image (stable and compatible with cp-kafka)
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
    healthcheck:
      test: ["CMD", "bash", "-lc", "nc -z localhost 2181 || exit 1"]

  kafka:
    # Use Confluent CP-Kafka image (compatible with Kafka 3.4+ via Confluent Platform 7.4)
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # Listen on all interfaces so containers can reach it. Advertise localhost for host clients
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      # Advertise the broker address that host processes (your producer running in the venv) can resolve.
      # For local development we advertise localhost:9092 so host clients connect successfully.
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    healthcheck:
      test: ["CMD", "bash", "-lc", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1" ]

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=crypto-cluster
    ports:
      - 9870:9870
    volumes:
      - hdfs_namenode:/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -f http://localhost:9870 || exit 1"]

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
    depends_on:
      - hadoop-namenode
    volumes:
      - hdfs_datanode:/hadoop/dfs/data

  # NOTE: Spark services temporarily disabled to avoid image/tag resolution failures during initial bring-up.
  # To enable Spark, restore a compatible Spark image and the service blocks for spark-master and spark-worker.

  mongo:
    image: mongo:6.0
    ports:
      - 27017:27017
    volumes:
      - mongo_data:/data/db
    command: ["mongod", "--bind_ip_all"]
    healthcheck:
      # Use mongosh for modern Mongo images (mongo shell has been removed in newer images)
      test: ["CMD", "bash", "-lc", "mongosh --quiet --eval 'db.adminCommand({ping:1})' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  backend:
    image: node:18
    working_dir: /usr/src/app
    volumes:
      - ../backend:/usr/src/app
    ports:
      - 3000:3000
    environment:
      - MONGO_URI=mongodb://mongo:27017
      - KAFKA_BOOTSTRAP=kafka:9092
    depends_on:
      - mongo
      - kafka
    command: ["bash", "-lc", "npm install --silent && npm run start"]

  frontend:
    image: node:18
    working_dir: /usr/src/app
    volumes:
      - ../frontend:/usr/src/app
    ports:
      - 3001:3000
    environment:
      - REACT_APP_API_URL=http://localhost:3000
    depends_on:
      - backend
    command: ["bash", "-lc", "npm install --silent && npm run start"]

volumes:
  hdfs_namenode:
  hdfs_datanode:
  mongo_data:
